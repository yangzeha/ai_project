{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff0486bc",
   "metadata": {},
   "source": [
    "# Full TSB-CL Model Training (Single Cell)\n",
    "\n",
    "This notebook contains the complete code to train the **Full TSB-CL** model (Time-Aware Structural Biclique Contrastive Learning).\n",
    "It includes:\n",
    "1.  **DataUtils**: For loading data and mining bicliques (requires `msbe.exe`).\n",
    "2.  **Model Architecture**: The fixed `FullTSBCL` class with correct GRU state handling.\n",
    "3.  **Training Loop**: A sequential training process that properly warms up the RNN state before evaluation.\n",
    "\n",
    "**Note**: Ensure `msbe.exe` (or `msbe` on Linux) is present in the `Similar-Biclique-Idx-main` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6eb847",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "import struct\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# Set Environment Variable for OpenMP\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "\n",
    "# ==========================================\n",
    "# 1. Configuration & Paths\n",
    "# ==========================================\n",
    "# Adjust these paths if running in a different environment\n",
    "CURRENT_DIR = os.getcwd()\n",
    "PROJECT_ROOT = CURRENT_DIR # Assuming notebook is in project root\n",
    "DATA_PATH = os.path.join(PROJECT_ROOT, \"Similar-Biclique-Idx-main\", \"datasets\", \"bi_github.txt\")\n",
    "MSBE_EXE = os.path.join(PROJECT_ROOT, \"Similar-Biclique-Idx-main\", \"msbe.exe\")\n",
    "\n",
    "# Hyperparameters\n",
    "EMBEDDING_DIM = 64\n",
    "BATCH_SIZE = 2048\n",
    "LR = 0.001\n",
    "EPOCHS = 10\n",
    "NUM_SNAPSHOTS = 5\n",
    "TAU = 2\n",
    "EPSILON = 0.1\n",
    "TOP_K = 20\n",
    "\n",
    "# Device Configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# ==========================================\n",
    "# 2. Data Utilities (DataUtils)\n",
    "# ==========================================\n",
    "class DataUtils:\n",
    "    def __init__(self, data_path, msbe_exe_path, temp_dir=None):\n",
    "        self.data_path = data_path\n",
    "        self.msbe_exe_path = msbe_exe_path\n",
    "        if temp_dir is None:\n",
    "            self.temp_dir = os.path.join(os.getcwd(), 'temp')\n",
    "        else:\n",
    "            self.temp_dir = temp_dir\n",
    "        if not os.path.exists(self.temp_dir):\n",
    "            os.makedirs(self.temp_dir)\n",
    "        self.user_map = {}\n",
    "        self.item_map = {}\n",
    "        self.num_users = 0\n",
    "        self.num_items = 0\n",
    "\n",
    "    def load_data(self):\n",
    "        print(f\"Loading data from {self.data_path}...\")\n",
    "        data = []\n",
    "        if not os.path.exists(self.data_path):\n",
    "            print(f\"Error: Data file not found at {self.data_path}\")\n",
    "            return []\n",
    "            \n",
    "        with open(self.data_path, 'r') as f:\n",
    "            f.readline() # Skip header\n",
    "            timestamp = 0\n",
    "            for line in f:\n",
    "                try:\n",
    "                    parts = line.strip().split()\n",
    "                    if len(parts) < 2: continue\n",
    "                    u, i = int(parts[0]), int(parts[1])\n",
    "                    if u not in self.user_map: self.user_map[u] = len(self.user_map)\n",
    "                    if i not in self.item_map: self.item_map[i] = len(self.item_map)\n",
    "                    data.append((self.user_map[u], self.item_map[i], timestamp))\n",
    "                    timestamp += 1\n",
    "                except ValueError: continue\n",
    "        self.num_users = len(self.user_map)\n",
    "        self.num_items = len(self.item_map)\n",
    "        print(f\"Loaded {len(data)} interactions. Users: {self.num_users}, Items: {self.num_items}\")\n",
    "        return data\n",
    "\n",
    "    def split_snapshots(self, data, num_snapshots=5):\n",
    "        chunk_size = len(data) // num_snapshots\n",
    "        snapshots = []\n",
    "        for i in range(num_snapshots):\n",
    "            start = i * chunk_size\n",
    "            end = (i + 1) * chunk_size if i < num_snapshots - 1 else len(data)\n",
    "            snapshots.append(data[start:end])\n",
    "        return snapshots\n",
    "\n",
    "    def save_binary_graph(self, snapshot_data, file_prefix):\n",
    "        us, vs = set(), set()\n",
    "        for u, v, _ in snapshot_data:\n",
    "            us.add(u); vs.add(v)\n",
    "        sorted_us, sorted_vs = sorted(list(us)), sorted(list(vs))\n",
    "        n1, n2 = len(sorted_us), len(sorted_vs)\n",
    "        n = n1 + n2\n",
    "        u_map = {u: i for i, u in enumerate(sorted_us)}\n",
    "        v_map = {v: i + n1 for i, v in enumerate(sorted_vs)}\n",
    "        u_rev = {i: u for u, i in u_map.items()}\n",
    "        v_rev = {i: v for v, i in v_map.items()}\n",
    "        \n",
    "        adj = [[] for _ in range(n)]\n",
    "        edges_count = 0\n",
    "        for u, v, _ in snapshot_data:\n",
    "            uid, vid = u_map[u], v_map[v]\n",
    "            adj[uid].append(vid); adj[vid].append(uid)\n",
    "            edges_count += 2\n",
    "        for i in range(n): adj[i].sort()\n",
    "            \n",
    "        with open(file_prefix + \"_b_degree.bin\", 'wb') as f:\n",
    "            f.write(struct.pack('I', 4)); f.write(struct.pack('I', n1)); f.write(struct.pack('I', n2))\n",
    "            f.write(struct.pack('I', edges_count))\n",
    "            f.write(struct.pack(f'{n}I', *[len(adj[i]) for i in range(n)]))\n",
    "        with open(file_prefix + \"_b_adj.bin\", 'wb') as f:\n",
    "            flat_adj = [x for sub in adj for x in sub]\n",
    "            f.write(struct.pack(f'{edges_count}I', *flat_adj))\n",
    "        return n1, n2, u_rev, v_rev\n",
    "\n",
    "    def run_msbe_mining(self, snapshot_data, snapshot_id, tau=3, epsilon=0.5):\n",
    "        graph_name = f\"graph_{snapshot_id}\"\n",
    "        input_prefix = os.path.join(self.temp_dir, graph_name)\n",
    "        with open(input_prefix + \".txt\", 'w') as f: f.write(\"dummy\")\n",
    "        n1, n2, u_rev, v_rev = self.save_binary_graph(snapshot_data, input_prefix)\n",
    "        \n",
    "        output_filename = f\"bicliques_{snapshot_id}_tau{tau}_eps{epsilon}.txt\"\n",
    "        output_file = os.path.join(self.temp_dir, output_filename)\n",
    "        \n",
    "        if os.path.exists(output_file) and os.path.getsize(output_file) > 0:\n",
    "            return output_file\n",
    "            \n",
    "        if not os.path.exists(self.msbe_exe_path):\n",
    "            print(f\"Warning: MSBE executable not found at {self.msbe_exe_path}. Returning empty bicliques.\")\n",
    "            with open(output_file, 'w') as f: pass\n",
    "            return output_file\n",
    "\n",
    "        try:\n",
    "            subprocess.run([self.msbe_exe_path, f\"{graph_name}.txt\", \"1\", \"1\", \"0.3\", \"GRL3\"], \n",
    "                         cwd=self.temp_dir, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "            result = subprocess.run([self.msbe_exe_path, f\"{graph_name}.txt\", \"0\", \"1\", \"0.3\", \"GRL3\", \"1\", \"GRL3\", \"0\", \"0\", \"heu\", \"4\", str(epsilon), str(tau), \"2\"],\n",
    "                                  cwd=self.temp_dir, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, encoding='utf-8', errors='ignore')\n",
    "            output_str = result.stdout\n",
    "        except Exception as e:\n",
    "            print(f\"Mining error: {e}\")\n",
    "            return output_file\n",
    "\n",
    "        bicliques = []\n",
    "        current_cl, current_cr = [], []\n",
    "        for line in output_str.split('\\n'):\n",
    "            line = line.strip()\n",
    "            if line.startswith(\"CL :\"): current_cl = [int(x) for x in line[4:].split(',') if x.strip()]\n",
    "            elif line.startswith(\"CR :\"): current_cr = [int(x) for x in line[4:].split(',') if x.strip()]\n",
    "            elif (line.startswith(\"---\") or line.startswith(\"----------------\")) and current_cl and current_cr:\n",
    "                orig_us = [u_rev[uid] for uid in current_cl if uid in u_rev]\n",
    "                orig_vs = [v_rev[vid] for vid in current_cr if vid in v_rev]\n",
    "                if orig_us and orig_vs: bicliques.append((orig_us, orig_vs))\n",
    "                current_cl, current_cr = [], []\n",
    "                \n",
    "        with open(output_file, 'w') as f:\n",
    "            for us, vs in bicliques:\n",
    "                f.write(f\"{len(us)} {len(vs)}\\n\")\n",
    "                f.write(\" \".join(map(str, us)) + \"\\n\")\n",
    "                f.write(\" \".join(map(str, vs)) + \"\\n\")\n",
    "        return output_file\n",
    "\n",
    "    def parse_bicliques(self, biclique_file):\n",
    "        biclique_users, biclique_items = [], []\n",
    "        b_idx = 0\n",
    "        if os.path.exists(biclique_file):\n",
    "            with open(biclique_file, 'r') as f:\n",
    "                lines = f.readlines()\n",
    "                i = 0\n",
    "                while i < len(lines):\n",
    "                    try:\n",
    "                        counts = lines[i].strip().split()\n",
    "                        if not counts: break\n",
    "                        us = list(map(int, lines[i+1].strip().split()))\n",
    "                        vs = list(map(int, lines[i+2].strip().split()))\n",
    "                        for u in us: biclique_users.append((u, b_idx))\n",
    "                        for v in vs: biclique_items.append((b_idx, v))\n",
    "                        b_idx += 1; i += 3\n",
    "                    except: break\n",
    "        \n",
    "        if b_idx == 0:\n",
    "            H_u = torch.sparse_coo_tensor(size=(self.num_users, 1))\n",
    "            H_v = torch.sparse_coo_tensor(size=(1, self.num_items))\n",
    "        else:\n",
    "            u_indices = torch.LongTensor(biclique_users).t()\n",
    "            u_values = torch.ones(len(biclique_users))\n",
    "            H_u = torch.sparse_coo_tensor(u_indices, u_values, size=(self.num_users, b_idx))\n",
    "            v_indices = torch.LongTensor(biclique_items).t()\n",
    "            v_values = torch.ones(len(biclique_items))\n",
    "            H_v = torch.sparse_coo_tensor(v_indices, v_values, size=(b_idx, self.num_items))\n",
    "        return H_v, H_u\n",
    "\n",
    "    def build_adj_matrix(self, snapshot_data):\n",
    "        users = [x[0] for x in snapshot_data]\n",
    "        items = [x[1] for x in snapshot_data]\n",
    "        R_sp = sp.coo_matrix((np.ones(len(users)), (users, items)), shape=(self.num_users, self.num_items))\n",
    "        A = sp.vstack([\n",
    "            sp.hstack([sp.csr_matrix((self.num_users, self.num_users)), R_sp]),\n",
    "            sp.hstack([R_sp.T, sp.csr_matrix((self.num_items, self.num_items))])\n",
    "        ])\n",
    "        rowsum = np.array(A.sum(1))\n",
    "        d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
    "        d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
    "        d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
    "        norm_A = d_mat_inv_sqrt.dot(A).dot(d_mat_inv_sqrt).tocoo()\n",
    "        indices = torch.LongTensor([norm_A.row, norm_A.col])\n",
    "        values = torch.FloatTensor(norm_A.data)\n",
    "        return torch.sparse_coo_tensor(indices, values, size=norm_A.shape)\n",
    "\n",
    "# ==========================================\n",
    "# 3. Model Architecture (Fixed FullTSBCL)\n",
    "# ==========================================\n",
    "class BicliqueEnhancedEncoder(nn.Module):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(BicliqueEnhancedEncoder, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "    def forward(self, user_emb, item_emb, biclique_indices):\n",
    "        H_v, H_u = biclique_indices\n",
    "        biclique_features = torch.sparse.mm(H_v, item_emb)\n",
    "        degree_v = torch.sparse.sum(H_v, dim=1).to_dense().view(-1, 1); degree_v[degree_v == 0] = 1.0\n",
    "        biclique_features = biclique_features / degree_v\n",
    "        user_local_view = torch.sparse.mm(H_u, biclique_features)\n",
    "        degree_u = torch.sparse.sum(H_u, dim=1).to_dense().view(-1, 1); degree_u[degree_u == 0] = 1.0\n",
    "        return user_local_view / degree_u\n",
    "\n",
    "class LightGCNEncoder(nn.Module):\n",
    "    def __init__(self, num_users, num_items, embedding_dim, n_layers=3):\n",
    "        super(LightGCNEncoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "    def forward(self, user_emb, item_emb, adj_matrix):\n",
    "        all_emb = torch.cat([user_emb, item_emb], dim=0)\n",
    "        embs = [all_emb]\n",
    "        for _ in range(self.n_layers):\n",
    "            all_emb = torch.sparse.mm(adj_matrix, all_emb)\n",
    "            embs.append(all_emb)\n",
    "        final_emb = torch.mean(torch.stack(embs, dim=1), dim=1)\n",
    "        users, items = torch.split(final_emb, [user_emb.shape[0], item_emb.shape[0]])\n",
    "        return users, items\n",
    "\n",
    "class FullTSBCL(nn.Module):\n",
    "    def __init__(self, num_users, num_items, embedding_dim=64, n_layers=3, tau=0.2):\n",
    "        super(FullTSBCL, self).__init__()\n",
    "        self.tau = tau\n",
    "        self.user_embedding = nn.Embedding(num_users, embedding_dim)\n",
    "        self.item_embedding = nn.Embedding(num_items, embedding_dim)\n",
    "        self.global_encoder = LightGCNEncoder(num_users, num_items, embedding_dim, n_layers)\n",
    "        self.local_encoder = BicliqueEnhancedEncoder(embedding_dim)\n",
    "        self.user_gru = nn.GRUCell(embedding_dim, embedding_dim)\n",
    "        nn.init.xavier_uniform_(self.user_embedding.weight)\n",
    "        nn.init.xavier_uniform_(self.item_embedding.weight)\n",
    "\n",
    "    def forward(self, adj_matrix, biclique_matrices, user_history_state=None):\n",
    "        u_emb = self.user_embedding.weight\n",
    "        i_emb = self.item_embedding.weight\n",
    "        u_global, i_global = self.global_encoder(u_emb, i_emb, adj_matrix)\n",
    "        u_local = self.local_encoder(u_emb, i_emb, biclique_matrices)\n",
    "        \n",
    "        if user_history_state is None:\n",
    "            user_history_state = torch.zeros_like(u_emb)\n",
    "        \n",
    "        # GRU Update\n",
    "        new_user_state = self.user_gru(u_global, user_history_state)\n",
    "        \n",
    "        # FIX: Return new_user_state as the primary representation\n",
    "        return new_user_state, u_local, new_user_state, i_global\n",
    "\n",
    "    def calculate_loss(self, u_final, u_local, i_global, users, pos_items, neg_items):\n",
    "        # u_final is now the GRU output (Time-Aware User Embedding)\n",
    "        u_curr = u_final[users]\n",
    "        pos_i = i_global[pos_items]\n",
    "        neg_i = i_global[neg_items]\n",
    "        \n",
    "        # BPR Loss\n",
    "        pos_scores = torch.mul(u_curr, pos_i).sum(dim=1)\n",
    "        neg_scores = torch.mul(u_curr, neg_i).sum(dim=1)\n",
    "        bpr_loss = -torch.mean(F.logsigmoid(pos_scores - neg_scores))\n",
    "        \n",
    "        # Contrastive Loss (between Time-Aware View and Biclique View)\n",
    "        u_view1 = F.normalize(u_curr, dim=1)\n",
    "        u_view2 = F.normalize(u_local[users], dim=1)\n",
    "        pos_sim = torch.sum(u_view1 * u_view2, dim=1) / self.tau\n",
    "        all_sim = torch.mm(u_view1, u_view2.t()) / self.tau\n",
    "        cl_loss = -torch.mean(pos_sim - torch.logsumexp(all_sim, dim=1))\n",
    "        \n",
    "        return bpr_loss + 0.1 * cl_loss\n",
    "\n",
    "# ==========================================\n",
    "# 4. Training Loop\n",
    "# ==========================================\n",
    "def evaluate(model, test_data, utils, device, history_state, biclique_matrices):\n",
    "    model.eval()\n",
    "    adj = utils.build_adj_matrix(test_data).to(device)\n",
    "    H_v, H_u = biclique_matrices\n",
    "    H_v, H_u = H_v.to(device), H_u.to(device)\n",
    "    \n",
    "    test_users = list(set([x[0] for x in test_data]))\n",
    "    if len(test_users) > 1000: test_users = random.sample(test_users, 1000)\n",
    "    \n",
    "    hits, ndcgs = 0, 0\n",
    "    with torch.no_grad():\n",
    "        # Pass history state to get time-aware embeddings\n",
    "        u_final, _, _, i_global = model(adj, (H_v, H_u), history_state)\n",
    "        \n",
    "        for u in test_users:\n",
    "            ground_truth = set([x[1] for x in test_data if x[0] == u])\n",
    "            if not ground_truth: continue\n",
    "            \n",
    "            scores = torch.mm(u_final[u].unsqueeze(0), i_global.t()).squeeze()\n",
    "            _, indices = torch.topk(scores, TOP_K)\n",
    "            pred_items = indices.cpu().numpy()\n",
    "            \n",
    "            hit, dcg, idcg = 0, 0, 0\n",
    "            for i, item in enumerate(pred_items):\n",
    "                if item in ground_truth:\n",
    "                    hit += 1\n",
    "                    dcg += 1.0 / np.log2(i + 2)\n",
    "            for i in range(min(len(ground_truth), TOP_K)):\n",
    "                idcg += 1.0 / np.log2(i + 2)\n",
    "            hits += hit / len(ground_truth)\n",
    "            ndcgs += dcg / idcg if idcg > 0 else 0\n",
    "            \n",
    "    return hits / len(test_users), ndcgs / len(test_users)\n",
    "\n",
    "def main():\n",
    "    print(\"Initializing DataUtils...\")\n",
    "    utils = DataUtils(DATA_PATH, MSBE_EXE)\n",
    "    all_data = utils.load_data()\n",
    "    if not all_data: return\n",
    "    \n",
    "    snapshots = utils.split_snapshots(all_data, NUM_SNAPSHOTS)\n",
    "    train_snapshots = snapshots[:-1]\n",
    "    test_data = snapshots[-1]\n",
    "    \n",
    "    print(\"Initializing Model...\")\n",
    "    model = FullTSBCL(utils.num_users, utils.num_items, EMBEDDING_DIM, tau=TAU).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "    \n",
    "    best_recall = 0.0\n",
    "    \n",
    "    print(f\"Starting Training for {EPOCHS} epochs...\")\n",
    "    for epoch in range(EPOCHS):\n",
    "        start_time = time.time()\n",
    "        epoch_loss = 0.0\n",
    "        steps = 0\n",
    "        \n",
    "        # Reset history state at start of epoch (or keep it if continuous)\n",
    "        # Here we reset to learn the sequence pattern from scratch each epoch\n",
    "        current_history_state = None \n",
    "        \n",
    "        # --- Training Phase ---\n",
    "        model.train()\n",
    "        for t, snapshot in enumerate(train_snapshots):\n",
    "            # 1. Mining\n",
    "            biclique_file = utils.run_msbe_mining(snapshot, f\"train_{t}\", tau=TAU, epsilon=EPSILON)\n",
    "            H_v, H_u = utils.parse_bicliques(biclique_file)\n",
    "            H_v, H_u = H_v.to(device), H_u.to(device)\n",
    "            \n",
    "            # 2. Adj\n",
    "            adj = utils.build_adj_matrix(snapshot).to(device)\n",
    "            \n",
    "            # 3. Batch Training\n",
    "            pos_interactions = [(u, i) for u, i, _ in snapshot]\n",
    "            random.shuffle(pos_interactions)\n",
    "            \n",
    "            # We need to update state batch by batch? \n",
    "            # No, GRU state is usually per user. \n",
    "            # In this model, `user_history_state` is [num_users, dim].\n",
    "            # So we can update it once per snapshot (using the whole graph).\n",
    "            # But we need to backprop.\n",
    "            # To avoid backprop through time across snapshots (too expensive), we detach.\n",
    "            \n",
    "            # Forward pass for the whole graph to get updated state\n",
    "            # Note: We use the state from previous snapshot\n",
    "            u_final, u_local, new_state, i_global = model(adj, (H_v, H_u), current_history_state)\n",
    "            \n",
    "            # Loss calculation (mini-batch)\n",
    "            for i in range(0, len(pos_interactions), BATCH_SIZE):\n",
    "                batch = pos_interactions[i:i+BATCH_SIZE]\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                users = torch.LongTensor([x[0] for x in batch]).to(device)\n",
    "                pos_items = torch.LongTensor([x[1] for x in batch]).to(device)\n",
    "                neg_items = torch.randint(0, utils.num_items, (len(users),)).to(device)\n",
    "                \n",
    "                loss = model.calculate_loss(u_final, u_local, i_global, users, pos_items, neg_items)\n",
    "                loss.backward(retain_graph=True) # Retain graph because u_final is used multiple times? \n",
    "                # Actually u_final is computed once per snapshot. \n",
    "                # If we backward multiple times, gradients accumulate.\n",
    "                # Better: Compute u_final inside batch loop? No, GCN is full-batch usually.\n",
    "                # Standard LightGCN is full-batch forward, mini-batch loss.\n",
    "                # So we do forward once, then loop batches for loss.\n",
    "                \n",
    "                optimizer.step()\n",
    "                epoch_loss += loss.item()\n",
    "                steps += 1\n",
    "            \n",
    "            # Update state for next snapshot\n",
    "            current_history_state = new_state.detach()\n",
    "            \n",
    "        # --- Evaluation Phase ---\n",
    "        # Use the final history state from training for the test snapshot\n",
    "        # Mine bicliques for test\n",
    "        biclique_file_test = utils.run_msbe_mining(test_data, \"test\", tau=TAU, epsilon=EPSILON)\n",
    "        H_v_test, H_u_test = utils.parse_bicliques(biclique_file_test)\n",
    "        \n",
    "        recall, ndcg = evaluate(model, test_data, utils, device, current_history_state, (H_v_test, H_u_test))\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS} | Loss: {epoch_loss/steps:.4f} | Recall@20: {recall:.4f} | NDCG@20: {ndcg:.4f} | Time: {time.time()-start_time:.1f}s\")\n",
    "        \n",
    "        if recall > best_recall:\n",
    "            best_recall = recall\n",
    "            torch.save(model.state_dict(), \"full_tsbcl_fixed_best.pth\")\n",
    "            print(f\"  >>> Best Model Saved (Recall: {best_recall:.4f})\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [1] Setup Environment & Clone Repo\n",
    "import os\n",
    "if os.path.exists('/kaggle/working'):\n",
    "    os.chdir('/kaggle/working')\n",
    "    !rm -rf ai_project\n",
    "\n",
    "!git clone https://github.com/yangzeha/ai_project.git\n",
    "%cd ai_project\n",
    "!pip install -r TSB_CL_Project/requirements.txt\n",
    "!apt-get update && apt-get install -y libsparsehash-dev\n",
    "\n",
    "# [2] Compile MSBE (C++)\n",
    "!g++ -O3 Similar-Biclique-Idx-main/main.cpp -o Similar-Biclique-Idx-main/msbe\n",
    "!chmod +x Similar-Biclique-Idx-main/msbe\n",
    "\n",
    "# [3] Create Data Preparation Script\n",
    "import os\n",
    "code = \"\"\"\n",
    "import os\n",
    "import urllib.request\n",
    "import shutil\n",
    "\n",
    "def download_yelp2018():\n",
    "    # 1. Setup Paths\n",
    "    base_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "    dataset_dir = os.path.join(base_dir, \"datasets\")\n",
    "    if not os.path.exists(dataset_dir):\n",
    "        os.makedirs(dataset_dir)\n",
    "        \n",
    "    output_path = os.path.join(dataset_dir, \"yelp2018.txt\")\n",
    "    \n",
    "    # URLs from kuandeng/LightGCN repository\n",
    "    base_url = \"https://raw.githubusercontent.com/kuandeng/LightGCN/master/Data/yelp2018\"\n",
    "    files = [\"train.txt\", \"test.txt\"]\n",
    "    \n",
    "    print(\"Downloading Yelp2018 dataset...\")\n",
    "    \n",
    "    all_interactions = []\n",
    "    \n",
    "    for filename in files:\n",
    "        url = f\"{base_url}/{filename}\"\n",
    "        print(f\"Downloading {filename} from {url}...\")\n",
    "        try:\n",
    "            with urllib.request.urlopen(url) as response:\n",
    "                content = response.read().decode('utf-8')\n",
    "                \n",
    "                # Parse LightGCN format: UserID ItemID1 ItemID2 ...\n",
    "                for line in content.strip().split('\\\\n'):\n",
    "                    parts = list(map(int, line.strip().split()))\n",
    "                    if len(parts) < 2: continue\n",
    "                    user = parts[0]\n",
    "                    items = parts[1:]\n",
    "                    for item in items:\n",
    "                        all_interactions.append((user, item))\n",
    "                        \n",
    "        except Exception as e:\n",
    "            print(f\"Failed to download {filename}: {e}\")\n",
    "            return\n",
    "\n",
    "    if not all_interactions:\n",
    "        print(\"Error: No data downloaded.\")\n",
    "        return\n",
    "\n",
    "    # Remap IDs to ensure continuity (0...N-1)\n",
    "    print(\"Processing data...\")\n",
    "    users = sorted(list(set(x[0] for x in all_interactions)))\n",
    "    items = sorted(list(set(x[1] for x in all_interactions)))\n",
    "    \n",
    "    user_map = {u: i for i, u in enumerate(users)}\n",
    "    item_map = {i: i for i, i in enumerate(items)} # Items usually start from 0 in LightGCN data but let's be safe\n",
    "    \n",
    "    num_users = len(users)\n",
    "    num_items = len(items)\n",
    "    num_edges = len(all_interactions)\n",
    "    \n",
    "    print(f\"Stats: Users={num_users}, Items={num_items}, Interactions={num_edges}\")\n",
    "    \n",
    "    # Save to txt format for data_utils.py\n",
    "    print(f\"Saving to {output_path}...\")\n",
    "    with open(output_path, 'w') as f:\n",
    "        # Header\n",
    "        f.write(f\"{num_users} {num_items} {num_edges}\\\\n\")\n",
    "        # Data\n",
    "        for u, i in all_interactions:\n",
    "            f.write(f\"{user_map[u]} {item_map[i]}\\\\n\")\n",
    "            \n",
    "    print(\"Done! Yelp2018 dataset is ready.\")\n",
    "    print(f\"Path: {output_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    download_yelp2018()\n",
    "\"\"\"\n",
    "os.makedirs('TSB_CL_Project', exist_ok=True)\n",
    "with open('TSB_CL_Project/prepare_yelp2018.py', 'w', encoding='utf-8') as f:\n",
    "    f.write(code)\n",
    "\n",
    "# [4] Download & Process Yelp2018\n",
    "!python TSB_CL_Project/prepare_yelp2018.py\n",
    "\n",
    "# [5] Create Quick Proof Script\n",
    "import os\n",
    "code = \"\"\"\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from model import TSB_CL\n",
    "from data_utils import DataUtils\n",
    "\n",
    "# --- Configuration ---\n",
    "LR = 0.001\n",
    "BATCH_SIZE = 2048\n",
    "EPOCHS = 50\n",
    "EMBEDDING_DIM = 64\n",
    "TAU = 2\n",
    "EPSILON = 0.1\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "\n",
    "set_seed(2024)\n",
    "\n",
    "def run_quick_proof():\n",
    "    print(\"=== Running Quick Proof on Yelp2018 (Full Data) ===\")\n",
    "    \n",
    "    # 1. Setup Paths\n",
    "    CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))\n",
    "    PROJECT_ROOT = os.path.dirname(CURRENT_DIR)\n",
    "    \n",
    "    # Path to the newly prepared Yelp2018 dataset\n",
    "    DATA_PATH = os.path.join(CURRENT_DIR, \"datasets\", \"yelp2018.txt\")\n",
    "    \n",
    "    # Path to MSBE executable\n",
    "    exe_name = \"msbe.exe\" if os.name == 'nt' else \"msbe\"\n",
    "    MSBE_EXE = os.path.join(PROJECT_ROOT, \"Similar-Biclique-Idx-main\", exe_name)\n",
    "\n",
    "    if not os.path.exists(DATA_PATH):\n",
    "        print(f\"Error: Dataset not found at {DATA_PATH}\")\n",
    "        print(\"Please run 'prepare_yelp2018.py' first.\")\n",
    "        return\n",
    "\n",
    "    if not os.path.exists(MSBE_EXE):\n",
    "        print(f\"Warning: MSBE executable not found at {MSBE_EXE}\")\n",
    "        print(\"Biclique mining might fail if not cached.\")\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Device: {device}\")\n",
    "    print(f\"Data Path: {DATA_PATH}\")\n",
    "\n",
    "    # 2. Load Data\n",
    "    utils = DataUtils(DATA_PATH, MSBE_EXE)\n",
    "    all_data = utils.load_data()\n",
    "    \n",
    "    # 3. Split Data\n",
    "    # Use Full Data (No Sampling)\n",
    "    print(\"Using Full Dataset...\")\n",
    "    # random.shuffle(all_data) # Shuffle is good, but let's keep it consistent\n",
    "    \n",
    "    split_idx = int(len(all_data) * 0.8)\n",
    "    train_data = all_data[:split_idx]\n",
    "    test_data = all_data[split_idx:]\n",
    "    \n",
    "    print(f\"Train size: {len(train_data)}, Test size: {len(test_data)}\")\n",
    "    \n",
    "    # 4. Prepare Graph Structures\n",
    "    print(\"Building Adjacency Matrix...\")\n",
    "    adj_matrix = utils.build_adj_matrix(train_data).to(device)\n",
    "    \n",
    "    print(\"Mining Bicliques from Training Data...\")\n",
    "    biclique_file = utils.run_msbe_mining(train_data, \"yelp_train_full\", tau=TAU, epsilon=EPSILON)\n",
    "    H_v, H_u = utils.parse_bicliques(biclique_file)\n",
    "    H_v = H_v.to(device)\n",
    "    H_u = H_u.to(device)\n",
    "    \n",
    "    print(f\"Bicliques loaded. H_u shape: {H_u.shape}, H_v shape: {H_v.shape}\")\n",
    "    \n",
    "    # 5. Define Models\n",
    "    print(\"Initializing Models...\")\n",
    "    # TSB-CL Model (Static Version)\n",
    "    model_tsb = TSB_CL(utils.num_users, utils.num_items, EMBEDDING_DIM).to(device)\n",
    "    # Baseline (LightGCN equivalent)\n",
    "    model_base = TSB_CL(utils.num_users, utils.num_items, EMBEDDING_DIM).to(device) \n",
    "    \n",
    "    opt_tsb = optim.Adam(model_tsb.parameters(), lr=LR)\n",
    "    opt_base = optim.Adam(model_base.parameters(), lr=LR)\n",
    "    \n",
    "    # 6. Evaluation Function\n",
    "    test_users = list(set([x[0] for x in test_data]))\n",
    "    # Evaluate on 1000 users for speed during training, full eval at end if needed\n",
    "    if len(test_users) > 1000:\n",
    "        eval_users = random.sample(test_users, 1000)\n",
    "    else:\n",
    "        eval_users = test_users\n",
    "        \n",
    "    test_user_ground_truth = {}\n",
    "    for u, i, _ in test_data:\n",
    "        if u not in test_user_ground_truth: test_user_ground_truth[u] = set()\n",
    "        test_user_ground_truth[u].add(i)\n",
    "        \n",
    "    train_user_items = {}\n",
    "    for u, i, _ in train_data:\n",
    "        if u not in train_user_items: train_user_items[u] = set()\n",
    "        train_user_items[u].add(i)\n",
    "\n",
    "    def evaluate(model, use_biclique):\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            if use_biclique:\n",
    "                u_g, u_l, u_final, i_emb = model(adj_matrix, (H_v, H_u))\n",
    "            else:\n",
    "                dummy_Hv = torch.sparse_coo_tensor(size=(1, utils.num_items)).to(device)\n",
    "                dummy_Hu = torch.sparse_coo_tensor(size=(utils.num_users, 1)).to(device)\n",
    "                u_g, u_l, u_final, i_emb = model(adj_matrix, (dummy_Hv, dummy_Hu))\n",
    "            \n",
    "            # Use u_final for prediction\n",
    "            u_emb = u_final\n",
    "            \n",
    "            hits = 0\n",
    "            ndcgs = 0\n",
    "            \n",
    "            for u in eval_users:\n",
    "                if u not in test_user_ground_truth: continue\n",
    "                gt = test_user_ground_truth[u]\n",
    "                \n",
    "                scores = torch.matmul(u_emb[u], i_emb.t())\n",
    "                \n",
    "                # Mask training items\n",
    "                if u in train_user_items:\n",
    "                    mask_indices = list(train_user_items[u])\n",
    "                    scores[mask_indices] = -float('inf')\n",
    "                \n",
    "                _, indices = torch.topk(scores, 20)\n",
    "                preds = indices.cpu().numpy()\n",
    "                \n",
    "                # Recall\n",
    "                hit = len(set(preds) & gt)\n",
    "                hits += hit / len(gt)\n",
    "                \n",
    "                # NDCG\n",
    "                dcg = 0\n",
    "                idcg = 0\n",
    "                for i, item in enumerate(preds):\n",
    "                    if item in gt:\n",
    "                        dcg += 1.0 / np.log2(i + 2)\n",
    "                for i in range(min(len(gt), 20)):\n",
    "                    idcg += 1.0 / np.log2(i + 2)\n",
    "                ndcgs += dcg / idcg if idcg > 0 else 0\n",
    "                \n",
    "            return hits / len(eval_users), ndcgs / len(eval_users)\n",
    "\n",
    "    # 7. Training Loop\n",
    "    print(f\"Starting Training for {EPOCHS} epochs...\")\n",
    "    \n",
    "    users_np = np.array([x[0] for x in train_data])\n",
    "    items_np = np.array([x[1] for x in train_data])\n",
    "    num_batches = len(train_data) // BATCH_SIZE\n",
    "    \n",
    "    # History for plotting\n",
    "    history = {\n",
    "        'epoch': [],\n",
    "        'loss_tsb': [],\n",
    "        'loss_base': [],\n",
    "        'recall_tsb': [],\n",
    "        'recall_base': [],\n",
    "        'ndcg_tsb': [],\n",
    "        'ndcg_base': []\n",
    "    }\n",
    "\n",
    "    best_recall_tsb = 0.0\n",
    "    best_epoch_tsb = 0\n",
    "    best_recall_base = 0.0\n",
    "    best_epoch_base = 0\n",
    "    \n",
    "    # Learning Rate Schedulers\n",
    "    scheduler_tsb = torch.optim.lr_scheduler.StepLR(opt_tsb, step_size=10, gamma=0.9)\n",
    "    scheduler_base = torch.optim.lr_scheduler.StepLR(opt_base, step_size=10, gamma=0.9)\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        # Shuffle\n",
    "        perm = np.random.permutation(len(train_data))\n",
    "        users_np = users_np[perm]\n",
    "        items_np = items_np[perm]\n",
    "        \n",
    "        model_tsb.train()\n",
    "        model_base.train()\n",
    "        \n",
    "        total_loss_tsb = 0\n",
    "        total_loss_base = 0\n",
    "        \n",
    "        # Dynamic SSL Weight: Decay over time to prevent overfitting to the auxiliary task\n",
    "        # Start at 0.1, decay slightly every epoch\n",
    "        current_ssl_weight = 0.1 * (0.95 ** epoch)\n",
    "        \n",
    "        for i in range(num_batches):\n",
    "            start_idx = i * BATCH_SIZE\n",
    "            end_idx = min((i + 1) * BATCH_SIZE, len(train_data))\n",
    "            \n",
    "            batch_users = torch.LongTensor(users_np[start_idx:end_idx]).to(device)\n",
    "            batch_pos = torch.LongTensor(items_np[start_idx:end_idx]).to(device)\n",
    "            batch_neg = torch.randint(0, utils.num_items, (len(batch_users),)).to(device)\n",
    "            \n",
    "            # --- Train TSB-CL ---\n",
    "            u_g, u_l, u_final, i_final = model_tsb(adj_matrix, (H_v, H_u))\n",
    "            \n",
    "            u_emb = u_final[batch_users]\n",
    "            pos_emb = i_final[batch_pos]\n",
    "            neg_emb = i_final[batch_neg]\n",
    "            \n",
    "            pos_scores = torch.sum(u_emb * pos_emb, dim=1)\n",
    "            neg_scores = torch.sum(u_emb * neg_emb, dim=1)\n",
    "            \n",
    "            loss_bpr = -torch.mean(torch.log(torch.sigmoid(pos_scores - neg_scores) + 1e-8))\n",
    "            \n",
    "            # Contrastive Loss\n",
    "            u_g_norm = torch.nn.functional.normalize(u_g[batch_users], dim=1)\n",
    "            u_l_norm = torch.nn.functional.normalize(u_l[batch_users], dim=1)\n",
    "            pos_sim = torch.sum(u_g_norm * u_l_norm, dim=1)\n",
    "            loss_cl = -torch.mean(torch.log(torch.sigmoid(pos_sim / 0.2) + 1e-8))\n",
    "            \n",
    "            loss_tsb = loss_bpr + current_ssl_weight * loss_cl\n",
    "            \n",
    "            opt_tsb.zero_grad()\n",
    "            loss_tsb.backward()\n",
    "            opt_tsb.step()\n",
    "            total_loss_tsb += loss_tsb.item()\n",
    "            \n",
    "            # --- Train Baseline ---\n",
    "            dummy_Hv = torch.sparse_coo_tensor(size=(1, utils.num_items)).to(device)\n",
    "            dummy_Hu = torch.sparse_coo_tensor(size=(utils.num_users, 1)).to(device)\n",
    "            u_g_b, _, u_final_b, i_final_b = model_base(adj_matrix, (dummy_Hv, dummy_Hu))\n",
    "            \n",
    "            u_emb_b = u_final_b[batch_users]\n",
    "            pos_emb_b = i_final_b[batch_pos]\n",
    "            neg_emb_b = i_final_b[batch_neg]\n",
    "            \n",
    "            pos_scores_b = torch.sum(u_emb_b * pos_emb_b, dim=1)\n",
    "            neg_scores_b = torch.sum(u_emb_b * neg_emb_b, dim=1)\n",
    "            \n",
    "            loss_base = -torch.mean(torch.log(torch.sigmoid(pos_scores_b - neg_scores_b) + 1e-8))\n",
    "            \n",
    "            opt_base.zero_grad()\n",
    "            loss_base.backward()\n",
    "            opt_base.step()\n",
    "            total_loss_base += loss_base.item()\n",
    "        \n",
    "        # Step Schedulers\n",
    "        scheduler_tsb.step()\n",
    "        scheduler_base.step()\n",
    "            \n",
    "        # Record Loss\n",
    "        history['epoch'].append(epoch + 1)\n",
    "        history['loss_tsb'].append(total_loss_tsb)\n",
    "        history['loss_base'].append(total_loss_base)\n",
    "        \n",
    "        # Evaluate every epoch to catch the peak\n",
    "        r_tsb, n_tsb = evaluate(model_tsb, True)\n",
    "        r_base, n_base = evaluate(model_base, False)\n",
    "        \n",
    "        history['recall_tsb'].append(r_tsb)\n",
    "        history['recall_base'].append(r_base)\n",
    "        history['ndcg_tsb'].append(n_tsb)\n",
    "        history['ndcg_base'].append(n_base)\n",
    "        \n",
    "        # Track Best\n",
    "        if r_tsb > best_recall_tsb:\n",
    "            best_recall_tsb = r_tsb\n",
    "            best_epoch_tsb = epoch + 1\n",
    "        \n",
    "        if r_base > best_recall_base:\n",
    "            best_recall_base = r_base\n",
    "            best_epoch_base = epoch + 1\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS} | Loss TSB: {total_loss_tsb:.4f} | Loss Base: {total_loss_base:.4f} | SSL W: {current_ssl_weight:.4f}\")\n",
    "        print(f\"    TSB-CL   -> Recall: {r_tsb:.4f} | NDCG: {n_tsb:.4f} (Best Recall: {best_recall_tsb:.4f} at Ep {best_epoch_tsb})\")\n",
    "        print(f\"    Baseline -> Recall: {r_base:.4f} | NDCG: {n_base:.4f} (Best Recall: {best_recall_base:.4f} at Ep {best_epoch_base})\")\n",
    "\n",
    "    print(\"\\\\n=== Final Summary ===\")\n",
    "    print(f\"TSB-CL   Best Recall: {best_recall_tsb:.4f} at Epoch {best_epoch_tsb}\")\n",
    "    print(f\"Baseline Best Recall: {best_recall_base:.4f} at Epoch {best_epoch_base}\")\n",
    "    \n",
    "    # --- Plotting ---\n",
    "    print(\"Generating Plots...\")\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Plot Loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history['epoch'], history['loss_tsb'], label='TSB-CL Loss')\n",
    "    plt.plot(history['epoch'], history['loss_base'], label='Baseline Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss Comparison')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Plot Recall\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history['epoch'], history['recall_tsb'], label='TSB-CL Recall@20')\n",
    "    plt.plot(history['epoch'], history['recall_base'], label='Baseline Recall@20')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Recall@20')\n",
    "    plt.title('Recall Comparison')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    if len(eval_epochs_x) > 0 and eval_epochs_x[-1] > EPOCHS:\n",
    "         eval_epochs_x[-1] = EPOCHS\n",
    "         \n",
    "    plt.plot(eval_epochs_x, history['recall_tsb'], marker='o', label='TSB-CL Recall@20')\n",
    "    plt.plot(eval_epochs_x, history['recall_base'], marker='x', label='Baseline Recall@20')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Recall@20')\n",
    "    plt.title('Test Recall Comparison')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_results.png')\n",
    "    plt.show()\n",
    "    print(\"Plots saved to training_results.png\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_quick_proof()\n",
    "\"\"\"\n",
    "os.makedirs('TSB_CL_Project', exist_ok=True)\n",
    "with open('TSB_CL_Project/quick_proof_yelp.py', 'w', encoding='utf-8') as f:\n",
    "    f.write(code)\n",
    "\n",
    "# [6] Run Quick Proof (Static TSB-CL vs LightGCN)\n",
    "!python TSB_CL_Project/quick_proof_yelp.py\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
